{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "VMZ1kEFj_hBe",
   "metadata": {
    "id": "VMZ1kEFj_hBe"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "v9qA4p1U_bav",
   "metadata": {
    "id": "v9qA4p1U_bav"
   },
   "source": [
    "# Procesamiento inicial de datos\n",
    "\n",
    "# **Limpeza de textos**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1179852d-47db-4b55-96e4-6a17fe8df906",
   "metadata": {
    "id": "1179852d-47db-4b55-96e4-6a17fe8df906"
   },
   "source": [
    "**Preinstalaciones**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "5c085037-5893-40a6-8c15-971bc808bc59",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18022,
     "status": "ok",
     "timestamp": 1721730365154,
     "user": {
      "displayName": "Inna Filatova",
      "userId": "01488652953098871073"
     },
     "user_tz": -120
    },
    "id": "5c085037-5893-40a6-8c15-971bc808bc59",
    "outputId": "6e788157-56a9-41e9-dab7-f18090089c59"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: emoji in /Applications/anaconda3/lib/python3.12/site-packages (2.12.1)\n",
      "Requirement already satisfied: typing-extensions>=4.7.0 in /Applications/anaconda3/lib/python3.12/site-packages (from emoji) (4.11.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langdetect in /Applications/anaconda3/lib/python3.12/site-packages (1.0.9)\n",
      "Requirement already satisfied: six in /Applications/anaconda3/lib/python3.12/site-packages (from langdetect) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /Applications/anaconda3/lib/python3.12/site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /Applications/anaconda3/lib/python3.12/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Applications/anaconda3/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Applications/anaconda3/lib/python3.12/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Applications/anaconda3/lib/python3.12/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /Applications/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /Applications/anaconda3/lib/python3.12/site-packages (3.7.5)\n",
      "Requirement already satisfied: pysentimiento in /Applications/anaconda3/lib/python3.12/site-packages (0.7.3)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Applications/anaconda3/lib/python3.12/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Applications/anaconda3/lib/python3.12/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Applications/anaconda3/lib/python3.12/site-packages (from spacy) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Applications/anaconda3/lib/python3.12/site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Applications/anaconda3/lib/python3.12/site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /Applications/anaconda3/lib/python3.12/site-packages (from spacy) (8.2.5)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Applications/anaconda3/lib/python3.12/site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Applications/anaconda3/lib/python3.12/site-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Applications/anaconda3/lib/python3.12/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /Applications/anaconda3/lib/python3.12/site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /Applications/anaconda3/lib/python3.12/site-packages (from spacy) (0.12.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Applications/anaconda3/lib/python3.12/site-packages (from spacy) (4.66.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Applications/anaconda3/lib/python3.12/site-packages (from spacy) (2.32.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Applications/anaconda3/lib/python3.12/site-packages (from spacy) (2.5.3)\n",
      "Requirement already satisfied: jinja2 in /Applications/anaconda3/lib/python3.12/site-packages (from spacy) (3.1.4)\n",
      "Requirement already satisfied: setuptools in /Applications/anaconda3/lib/python3.12/site-packages (from spacy) (69.5.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /Applications/anaconda3/lib/python3.12/site-packages (from spacy) (23.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Applications/anaconda3/lib/python3.12/site-packages (from spacy) (3.4.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /Applications/anaconda3/lib/python3.12/site-packages (from spacy) (1.26.4)\n",
      "Requirement already satisfied: accelerate>=0.27.2 in /Applications/anaconda3/lib/python3.12/site-packages (from pysentimiento) (0.33.0)\n",
      "Requirement already satisfied: datasets>=2.10.1 in /Applications/anaconda3/lib/python3.12/site-packages (from pysentimiento) (2.20.0)\n",
      "Requirement already satisfied: emoji>=1.6.1 in /Applications/anaconda3/lib/python3.12/site-packages (from pysentimiento) (2.12.1)\n",
      "Requirement already satisfied: torch!=2.0.1,>=2.0.0 in /Applications/anaconda3/lib/python3.12/site-packages (from pysentimiento) (2.4.0)\n",
      "Requirement already satisfied: transformers>=4.13.0 in /Applications/anaconda3/lib/python3.12/site-packages (from pysentimiento) (4.44.0)\n",
      "Requirement already satisfied: psutil in /Applications/anaconda3/lib/python3.12/site-packages (from accelerate>=0.27.2->pysentimiento) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in /Applications/anaconda3/lib/python3.12/site-packages (from accelerate>=0.27.2->pysentimiento) (6.0.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /Applications/anaconda3/lib/python3.12/site-packages (from accelerate>=0.27.2->pysentimiento) (0.24.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Applications/anaconda3/lib/python3.12/site-packages (from accelerate>=0.27.2->pysentimiento) (0.4.3)\n",
      "Requirement already satisfied: filelock in /Applications/anaconda3/lib/python3.12/site-packages (from datasets>=2.10.1->pysentimiento) (3.13.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /Applications/anaconda3/lib/python3.12/site-packages (from datasets>=2.10.1->pysentimiento) (17.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /Applications/anaconda3/lib/python3.12/site-packages (from datasets>=2.10.1->pysentimiento) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Applications/anaconda3/lib/python3.12/site-packages (from datasets>=2.10.1->pysentimiento) (0.3.8)\n",
      "Requirement already satisfied: pandas in /Applications/anaconda3/lib/python3.12/site-packages (from datasets>=2.10.1->pysentimiento) (2.2.2)\n",
      "Requirement already satisfied: xxhash in /Applications/anaconda3/lib/python3.12/site-packages (from datasets>=2.10.1->pysentimiento) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /Applications/anaconda3/lib/python3.12/site-packages (from datasets>=2.10.1->pysentimiento) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in /Applications/anaconda3/lib/python3.12/site-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets>=2.10.1->pysentimiento) (2024.3.1)\n",
      "Requirement already satisfied: aiohttp in /Applications/anaconda3/lib/python3.12/site-packages (from datasets>=2.10.1->pysentimiento) (3.9.5)\n",
      "Requirement already satisfied: typing-extensions>=4.7.0 in /Applications/anaconda3/lib/python3.12/site-packages (from emoji>=1.6.1->pysentimiento) (4.11.0)\n",
      "Requirement already satisfied: language-data>=1.2 in /Applications/anaconda3/lib/python3.12/site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Applications/anaconda3/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.6 in /Applications/anaconda3/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.14.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Applications/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Applications/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Applications/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Applications/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.6.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Applications/anaconda3/lib/python3.12/site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Applications/anaconda3/lib/python3.12/site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
      "Requirement already satisfied: sympy in /Applications/anaconda3/lib/python3.12/site-packages (from torch!=2.0.1,>=2.0.0->pysentimiento) (1.13.1)\n",
      "Requirement already satisfied: networkx in /Applications/anaconda3/lib/python3.12/site-packages (from torch!=2.0.1,>=2.0.0->pysentimiento) (3.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Applications/anaconda3/lib/python3.12/site-packages (from transformers>=4.13.0->pysentimiento) (2024.5.15)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /Applications/anaconda3/lib/python3.12/site-packages (from transformers>=4.13.0->pysentimiento) (0.19.1)\n",
      "Requirement already satisfied: click>=8.0.0 in /Applications/anaconda3/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /Applications/anaconda3/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /Applications/anaconda3/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.7.1)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /Applications/anaconda3/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.18.1)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /Applications/anaconda3/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Applications/anaconda3/lib/python3.12/site-packages (from jinja2->spacy) (2.1.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Applications/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets>=2.10.1->pysentimiento) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Applications/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets>=2.10.1->pysentimiento) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Applications/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets>=2.10.1->pysentimiento) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Applications/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets>=2.10.1->pysentimiento) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Applications/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets>=2.10.1->pysentimiento) (1.9.3)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in /Applications/anaconda3/lib/python3.12/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Applications/anaconda3/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Applications/anaconda3/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.15.1)\n",
      "Requirement already satisfied: wrapt in /Applications/anaconda3/lib/python3.12/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Applications/anaconda3/lib/python3.12/site-packages (from pandas->datasets>=2.10.1->pysentimiento) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Applications/anaconda3/lib/python3.12/site-packages (from pandas->datasets>=2.10.1->pysentimiento) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Applications/anaconda3/lib/python3.12/site-packages (from pandas->datasets>=2.10.1->pysentimiento) (2023.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Applications/anaconda3/lib/python3.12/site-packages (from sympy->torch!=2.0.1,>=2.0.0->pysentimiento) (1.3.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Applications/anaconda3/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
      "Requirement already satisfied: six>=1.5 in /Applications/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.10.1->pysentimiento) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting es-core-news-md==3.7.0\n",
      "  Using cached https://github.com/explosion/spacy-models/releases/download/es_core_news_md-3.7.0/es_core_news_md-3.7.0-py3-none-any.whl (42.3 MB)\n",
      "Requirement already satisfied: spacy<3.8.0,>=3.7.0 in /Applications/anaconda3/lib/python3.12/site-packages (from es-core-news-md==3.7.0) (3.7.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Applications/anaconda3/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Applications/anaconda3/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Applications/anaconda3/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Applications/anaconda3/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Applications/anaconda3/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /Applications/anaconda3/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (8.2.5)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Applications/anaconda3/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Applications/anaconda3/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Applications/anaconda3/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /Applications/anaconda3/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /Applications/anaconda3/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (0.12.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Applications/anaconda3/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (4.66.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Applications/anaconda3/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (2.32.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Applications/anaconda3/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (2.5.3)\n",
      "Requirement already satisfied: jinja2 in /Applications/anaconda3/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (3.1.4)\n",
      "Requirement already satisfied: setuptools in /Applications/anaconda3/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (69.5.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /Applications/anaconda3/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (23.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Applications/anaconda3/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (3.4.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /Applications/anaconda3/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (1.26.4)\n",
      "Requirement already satisfied: language-data>=1.2 in /Applications/anaconda3/lib/python3.12/site-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (1.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Applications/anaconda3/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.6 in /Applications/anaconda3/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (2.14.6)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /Applications/anaconda3/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Applications/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Applications/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Applications/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Applications/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (2024.6.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Applications/anaconda3/lib/python3.12/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Applications/anaconda3/lib/python3.12/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (0.1.5)\n",
      "Requirement already satisfied: click>=8.0.0 in /Applications/anaconda3/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /Applications/anaconda3/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /Applications/anaconda3/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (13.7.1)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /Applications/anaconda3/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (0.18.1)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /Applications/anaconda3/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (7.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Applications/anaconda3/lib/python3.12/site-packages (from jinja2->spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (2.1.3)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in /Applications/anaconda3/lib/python3.12/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (1.2.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Applications/anaconda3/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Applications/anaconda3/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (2.15.1)\n",
      "Requirement already satisfied: wrapt in /Applications/anaconda3/lib/python3.12/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (1.16.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Applications/anaconda3/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (0.1.2)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('es_core_news_md')\n"
     ]
    }
   ],
   "source": [
    "!pip install emoji\n",
    "!pip install langdetect\n",
    "!pip install pandas \n",
    "\n",
    "!pip install spacy pysentimiento\n",
    "!python -m spacy download es_core_news_md #Modelo SpaCy para español "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "BS0g_kvGMzEq",
   "metadata": {
    "id": "BS0g_kvGMzEq"
   },
   "source": [
    "**Importaciones**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "JtqyiJjgHrLc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 589
    },
    "executionInfo": {
     "elapsed": 256,
     "status": "ok",
     "timestamp": 1721731573963,
     "user": {
      "displayName": "Inna Filatova",
      "userId": "01488652953098871073"
     },
     "user_tz": -120
    },
    "id": "JtqyiJjgHrLc",
    "outputId": "173f9e2a-8ffa-4af2-9152-76fab0129df1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndf = pd.read_csv(\"/reviews_filtered_prueba.csv\") #impotar CSVs sujetos a la limpieza\\ndisplay(df)\\n\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import emoji\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "'''\n",
    "df = pd.read_csv(\"/reviews_filtered_prueba.csv\") #impotar CSVs sujetos a la limpieza\n",
    "display(df)\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3PoAycG6OuuV",
   "metadata": {
    "id": "3PoAycG6OuuV"
   },
   "source": [
    "**Filtrar reseñas por idioma, elimirar no españolas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "IQRTyUU0K98P",
   "metadata": {
    "executionInfo": {
     "elapsed": 9597,
     "status": "ok",
     "timestamp": 1721730379107,
     "user": {
      "displayName": "Inna Filatova",
      "userId": "01488652953098871073"
     },
     "user_tz": -120
    },
    "id": "IQRTyUU0K98P"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing local_IDs for 0.csv: 100%|█████████| 279/279 [00:34<00:00,  7.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Reviews left in the Cafeteras de goteo category: 20427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing local_IDs for 1.csv: 100%|███████████| 98/98 [00:28<00:00,  3.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Reviews left in the Cafeteras automaticas category: 16052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing local_IDs for 2.csv: 100%|█████████| 148/148 [00:33<00:00,  4.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Reviews left in the Cafeteras individuales category: 19016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Limpiar y extraer reseñas no españolas\n",
    "\n",
    "from langdetect import detect\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "def is_spanish(text):\n",
    "    try:\n",
    "        return detect(text) == 'es'\n",
    "    except: #en caso de error o idioma no detectado\n",
    "        return False\n",
    "\n",
    "def filter_reseñas_esp(filename):\n",
    "    df = pd.read_csv(filename)\n",
    "    count_original = df.shape[0]\n",
    "    \n",
    "    filtered_df = df[df[\"body\"].apply(is_spanish)]\n",
    "    filtered_df.to_csv(filename, index = False, encoding = \"utf-8\")\n",
    "    count_final = filtered_df.shape[0]\n",
    "\n",
    "    diff = count_original - count_final \n",
    "    return (filtered_df, diff)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    base_df = pd.read_csv('new_database/base.csv')\n",
    "\n",
    "    dataframes = {}\n",
    "\n",
    "    for ID in base_df['ID']:\n",
    "        id_df = pd.read_csv(f'new_database/{ID}.csv')\n",
    "        category = base_df.loc[base_df['ID'] == ID, 'category'].values\n",
    "\n",
    "        reviews_df = pd.DataFrame()\n",
    "\n",
    "        for local_id in tqdm(id_df['ID'], desc=f\"Processing local_IDs for {ID}.csv\"):\n",
    "            review_filename = f'new_database/reviews/{local_id}.csv'\n",
    "        \n",
    "            (filtered_df, count) = filter_reseñas_esp(review_filename)\n",
    "            #print(f'Deleted {count} reviews in {review_filename}')\n",
    "\n",
    "            reviews_df = pd.concat([reviews_df, filtered_df], ignore_index=True)\n",
    "        \n",
    "        print('-'*50 + f'\\nReviews left in the {category[0]} category: {reviews_df.shape[0]}')\n",
    "        dataframes[ID] = reviews_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "53t4bzRk_uNv",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 589
    },
    "executionInfo": {
     "elapsed": 433,
     "status": "ok",
     "timestamp": 1721730392591,
     "user": {
      "displayName": "Inna Filatova",
      "userId": "01488652953098871073"
     },
     "user_tz": -120
    },
    "id": "53t4bzRk_uNv",
    "outputId": "c87f6891-8e59-4fa2-a109-f813a5d94d39"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>rating</th>\n",
       "      <th>body</th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>No ha durado ni la garantía + pésimo servicio ...</td>\n",
       "      <td>1,0 de 5 estrellas</td>\n",
       "      <td>Ni los 24 meses de garantía ha durado, y al en...</td>\n",
       "      <td>MaX</td>\n",
       "      <td>Revisado en España el 3 de abril de 2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Está bien</td>\n",
       "      <td>3,0 de 5 estrellas</td>\n",
       "      <td>Después de más de dos meses usándola hace muy ...</td>\n",
       "      <td>Lorena</td>\n",
       "      <td>Revisado en España el 14 de marzo de 2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Perfecta!</td>\n",
       "      <td>5,0 de 5 estrellas</td>\n",
       "      <td>Me encanta! Soy adicta al café y me gusta toma...</td>\n",
       "      <td>Perfecta!</td>\n",
       "      <td>Revisado en España el 22 de febrero de 2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EXCELENTE</td>\n",
       "      <td>5,0 de 5 estrellas</td>\n",
       "      <td>como única pega que limpiar el palito del vapo...</td>\n",
       "      <td>Sil at</td>\n",
       "      <td>Revisado en España el 20 de febrero de 2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Un valle realmente bueno por el precio relativ...</td>\n",
       "      <td>5,0 de 5 estrellas</td>\n",
       "      <td>Un valle realmente bueno por el precio relativ...</td>\n",
       "      <td>Sylvie O'Connell</td>\n",
       "      <td>Revisado en España el 5 de febrero de 2024</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title              rating  \\\n",
       "0  No ha durado ni la garantía + pésimo servicio ...  1,0 de 5 estrellas   \n",
       "1                                          Está bien  3,0 de 5 estrellas   \n",
       "2                                          Perfecta!  5,0 de 5 estrellas   \n",
       "3                                          EXCELENTE  5,0 de 5 estrellas   \n",
       "4  Un valle realmente bueno por el precio relativ...  5,0 de 5 estrellas   \n",
       "\n",
       "                                                body            author  \\\n",
       "0  Ni los 24 meses de garantía ha durado, y al en...               MaX   \n",
       "1  Después de más de dos meses usándola hace muy ...            Lorena   \n",
       "2  Me encanta! Soy adicta al café y me gusta toma...         Perfecta!   \n",
       "3  como única pega que limpiar el palito del vapo...            Sil at   \n",
       "4  Un valle realmente bueno por el precio relativ...  Sylvie O'Connell   \n",
       "\n",
       "                                          date  \n",
       "0     Revisado en España el 3 de abril de 2024  \n",
       "1    Revisado en España el 14 de marzo de 2024  \n",
       "2  Revisado en España el 22 de febrero de 2024  \n",
       "3  Revisado en España el 20 de febrero de 2024  \n",
       "4   Revisado en España el 5 de febrero de 2024  "
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Salida de reseñas detectadas como españolas, mostrar primeras cinco reseñas\n",
    "\n",
    "dataframes = []\n",
    "for i in range (3): \n",
    "    dataframes.append(pd.read_csv(f\"category_{i}.csv\"))\n",
    "dataframes[0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0si415JeOlD4",
   "metadata": {
    "id": "0si415JeOlD4"
   },
   "source": [
    "**Limpieza**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "c07f5bd4-1e9b-467c-b42d-b3aaec8ed68d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 226,
     "status": "ok",
     "timestamp": 1721730423515,
     "user": {
      "displayName": "Inna Filatova",
      "userId": "01488652953098871073"
     },
     "user_tz": -120
    },
    "id": "c07f5bd4-1e9b-467c-b42d-b3aaec8ed68d",
    "outputId": "6880af97-3a44-45a6-c1e1-e87da7bbc11b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:4: SyntaxWarning: invalid escape sequence '\\]'\n",
      "<>:4: SyntaxWarning: invalid escape sequence '\\]'\n",
      "/var/folders/yz/8sxcfyl57n5459dd5zkccpnh0000gq/T/ipykernel_57731/504293372.py:4: SyntaxWarning: invalid escape sequence '\\]'\n",
      "  puntuacion = '¡!\"$%&\\'()*+,-./:;<=>¿?[\\]^_`{|}~'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' hola   que tal   no puedo venir  pero te envío un regalo'"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Comando para quitar puntuación en el texto\n",
    "\n",
    "def quitar_puntuacion(texto):\n",
    "  puntuacion = '¡!\"$%&\\'()*+,-./:;<=>¿?[\\]^_`{|}~'\n",
    "  return re.sub('[%s]' % re.escape(puntuacion), ' ', texto)\n",
    "\n",
    "#Ejemplo de quitar puntuación\n",
    "\n",
    "quitar_puntuacion(\"¡hola! ¿que tal? +no puedo venir, pero te envío un regalo\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "lHwlT78CJwsO",
   "metadata": {
    "executionInfo": {
     "elapsed": 248,
     "status": "ok",
     "timestamp": 1721740520802,
     "user": {
      "displayName": "Inna Filatova",
      "userId": "01488652953098871073"
     },
     "user_tz": -120
    },
    "id": "lHwlT78CJwsO"
   },
   "outputs": [],
   "source": [
    "#Quitar puntuación en reseñas filtradas\n",
    "\n",
    "filtered_df['body'] = filtered_df['body'].apply(quitar_puntuacion)\n",
    "for i in range (3): #recorre los 3 csvs que correspondes a las tres categorías de cafeteras\n",
    "    dataframes[i]['body']= dataframes[i]['body'].apply(quitar_puntuacion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "DT398TglHBeL",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 231,
     "status": "ok",
     "timestamp": 1721740558663,
     "user": {
      "displayName": "Inna Filatova",
      "userId": "01488652953098871073"
     },
     "user_tz": -120
    },
    "id": "DT398TglHBeL",
    "outputId": "d1ae42e1-9735-4338-ce34-771e20be2701"
   },
   "outputs": [],
   "source": [
    "#Resultado\n",
    "dataframes[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "76763d1e-c04e-496f-ae18-50893b7e4f2e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 256,
     "status": "ok",
     "timestamp": 1721730869875,
     "user": {
      "displayName": "Inna Filatova",
      "userId": "01488652953098871073"
     },
     "user_tz": -120
    },
    "id": "76763d1e-c04e-496f-ae18-50893b7e4f2e",
    "outputId": "8000d978-032a-4110-f965-934b3dfb6118"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Como lo habeis pasado, si ,que'"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Comando para quitar acentos gráficos\n",
    "\n",
    "def normalizar_tildes(texto):\n",
    "    a = 'áàäâ'\n",
    "    e = 'éèëê'\n",
    "    i = 'íìïî'\n",
    "    o = 'óòöô'\n",
    "    u = 'úùüû'\n",
    "    texto_normal = re.sub('[%s]' % re.escape(a), 'a', texto)\n",
    "    texto_normal = re.sub('[%s]' % re.escape(e), 'e', texto_normal)\n",
    "    texto_normal = re.sub('[%s]' % re.escape(i), 'i', texto_normal)\n",
    "    texto_normal = re.sub('[%s]' % re.escape(o), 'o', texto_normal)\n",
    "    texto_normal = re.sub('[%s]' % re.escape(u), 'u', texto_normal)\n",
    "    return texto_normal\n",
    "\n",
    "#Ejemplo de quitar acentos gráficos\n",
    "\n",
    "normalizar_tildes (\"Cómo lo habéis pasado, sí ,qué\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "V5OO99YWIqdS",
   "metadata": {
    "executionInfo": {
     "elapsed": 222,
     "status": "ok",
     "timestamp": 1721742590219,
     "user": {
      "displayName": "Inna Filatova",
      "userId": "01488652953098871073"
     },
     "user_tz": -120
    },
    "id": "V5OO99YWIqdS"
   },
   "outputs": [],
   "source": [
    "#Quitar acentos gráficos en reseñas filtradas\n",
    "\n",
    "filtered_df['body'] = filtered_df['body'].apply(normalizar_tildes)\n",
    "for i in range (3):\n",
    "    dataframes[i]['body']= dataframes[i]['body'].apply(normalizar_tildes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "Uh86HB8b2taI",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 223,
     "status": "ok",
     "timestamp": 1721742597040,
     "user": {
      "displayName": "Inna Filatova",
      "userId": "01488652953098871073"
     },
     "user_tz": -120
    },
    "id": "Uh86HB8b2taI",
    "outputId": "12f25951-0ed6-4e80-c95d-03d957d6d48b"
   },
   "outputs": [],
   "source": [
    "#Resultado\n",
    "dataframes[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "0ecc138f-8f1a-401e-a5bd-3a072a6c2a37",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 222,
     "status": "ok",
     "timestamp": 1721731198406,
     "user": {
      "displayName": "Inna Filatova",
      "userId": "01488652953098871073"
     },
     "user_tz": -120
    },
    "id": "0ecc138f-8f1a-401e-a5bd-3a072a6c2a37",
    "outputId": "83074468-c32c-4bca-e21a-8878518a6a5d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'quiero ver la tele'"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Comando para sustituir mayúsculas por minúsculas\n",
    "\n",
    "def sustituir_minusculas(texto):\n",
    "    texto_letras_min = texto.lower()\n",
    "    return texto_letras_min\n",
    "\n",
    "#Ejemplo de sustituir mayúsculas por minúsculas\n",
    "\n",
    "sustituir_minusculas (\"QuieRo Ver La TeLE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "4mhnKMxALULy",
   "metadata": {
    "executionInfo": {
     "elapsed": 252,
     "status": "ok",
     "timestamp": 1721742662625,
     "user": {
      "displayName": "Inna Filatova",
      "userId": "01488652953098871073"
     },
     "user_tz": -120
    },
    "id": "4mhnKMxALULy"
   },
   "outputs": [],
   "source": [
    "#Sustituir mayúsculas por minúsculas en reseñas filtradas\n",
    "\n",
    "filtered_df['body'] = filtered_df['body'].apply(sustituir_minusculas)\n",
    "for i in range (3):\n",
    "    dataframes[i]['body']= dataframes[i]['body'].apply(sustituir_minusculas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "swoFBxek3A7y",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 304,
     "status": "ok",
     "timestamp": 1721742665854,
     "user": {
      "displayName": "Inna Filatova",
      "userId": "01488652953098871073"
     },
     "user_tz": -120
    },
    "id": "swoFBxek3A7y",
    "outputId": "eeb86744-ef38-4a17-e3b4-2981713a8d16"
   },
   "outputs": [],
   "source": [
    "#Resultado\n",
    "dataframes[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "901241e3-038e-4748-82c8-d1fbf3f95882",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 224,
     "status": "ok",
     "timestamp": 1721731355328,
     "user": {
      "displayName": "Inna Filatova",
      "userId": "01488652953098871073"
     },
     "user_tz": -120
    },
    "id": "901241e3-038e-4748-82c8-d1fbf3f95882",
    "outputId": "5843cd20-c41d-4206-f455-c1d73b8a02bc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'He comprado  cafeteras, la  funcionó solo  horas'"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Comando para quitar números\n",
    "\n",
    "def quitar_numeros(texto):\n",
    "    texto_sin_numeros = re.sub(r'\\d+', '', texto)\n",
    "    return texto_sin_numeros\n",
    "\n",
    "#Ejemplo de quitar números\n",
    "\n",
    "quitar_numeros (\"He comprado 2 cafeteras, la 1 funcionó solo 24 horas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "b2Ios_aiL7Kj",
   "metadata": {
    "executionInfo": {
     "elapsed": 248,
     "status": "ok",
     "timestamp": 1721742703026,
     "user": {
      "displayName": "Inna Filatova",
      "userId": "01488652953098871073"
     },
     "user_tz": -120
    },
    "id": "b2Ios_aiL7Kj"
   },
   "outputs": [],
   "source": [
    "#Quitar números en reseñas filtradas\n",
    "\n",
    "filtered_df['body'] = filtered_df['body'].apply(quitar_numeros)\n",
    "for i in range (3):\n",
    "    dataframes[i]['body']= dataframes[i]['body'].apply(quitar_numeros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "SlXPHKju3JtX",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 238,
     "status": "ok",
     "timestamp": 1721742704752,
     "user": {
      "displayName": "Inna Filatova",
      "userId": "01488652953098871073"
     },
     "user_tz": -120
    },
    "id": "SlXPHKju3JtX",
    "outputId": "4fa4627b-fa0c-40f3-bbe8-99eca30e6e62"
   },
   "outputs": [],
   "source": [
    "#Resultado\n",
    "dataframes[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "yqdd9p67iVZY",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 278,
     "status": "ok",
     "timestamp": 1721737567475,
     "user": {
      "displayName": "Inna Filatova",
      "userId": "01488652953098871073"
     },
     "user_tz": -120
    },
    "id": "yqdd9p67iVZY",
    "outputId": "eb4cd6c0-10db-4abb-cfb1-161fcd34d4ff"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'No me vale porque, soy de , además '"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Comando para quitar emojis con UNICODE\n",
    "\n",
    "import regex as re    #regex para encontrar emojis\n",
    "def quitar_emojis(texto):\n",
    "    emoji_pattern = re.compile(\n",
    "        u'['\n",
    "        u'\\U0001F600-\\U0001F64F'  # Emoticonos\n",
    "        u'\\U0001F300-\\U0001F5FF'  # Símbolos y pictogramas\n",
    "        u'\\U0001F680-\\U0001F6FF'  # Transporte y símbolos de mapa\n",
    "        u'\\U0001F700-\\U0001F77F'  # Símbolos alchemicos\n",
    "        u'\\U0001F780-\\U0001F7FF'  # Símbolos geométricos adicionales\n",
    "        u'\\U0001F800-\\U0001F8FF'  # Símbolos adicionales de flechas\n",
    "        u'\\U0001F900-\\U0001F9FF'  # Emojis adicionales\n",
    "        u'\\U0001FA00-\\U0001FA6F'  # Más emojis adicionales\n",
    "        u'\\U0001FA70-\\U0001FAFF'  # Más emojis adicionales\n",
    "        u'\\U00002702-\\U000027B0'  # Símbolos de soporte\n",
    "        u'\\U000024C2-\\U0001F251'\n",
    "        u']+', flags=re.UNICODE)\n",
    "\n",
    "    texto_sin_emoji = re.sub(emoji_pattern, '', texto)\n",
    "    return texto_sin_emoji\n",
    "\n",
    "#Ejemplo de quitar emojis\n",
    "\n",
    "quitar_emojis(\"No me vale 😆🐻🤣😂porque👌🙂🚑, soy de 🇪🇸, además ❤️➡️\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "TMaLnirQ3PcM",
   "metadata": {
    "executionInfo": {
     "elapsed": 252,
     "status": "ok",
     "timestamp": 1721742748697,
     "user": {
      "displayName": "Inna Filatova",
      "userId": "01488652953098871073"
     },
     "user_tz": -120
    },
    "id": "TMaLnirQ3PcM"
   },
   "outputs": [],
   "source": [
    "#Quitar emojis en reseñas filtradas\n",
    "\n",
    "filtered_df['body'] = filtered_df['body'].apply(quitar_emojis)\n",
    "for i in range (3):\n",
    "    dataframes[i]['body']= dataframes[i]['body'].apply(quitar_emojis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "HFN-T1AL3WKj",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 235,
     "status": "ok",
     "timestamp": 1721742750621,
     "user": {
      "displayName": "Inna Filatova",
      "userId": "01488652953098871073"
     },
     "user_tz": -120
    },
    "id": "HFN-T1AL3WKj",
    "outputId": "29a7ffab-7463-4bae-d772-a36f9c6d7f13"
   },
   "outputs": [],
   "source": [
    "#Resultado\n",
    "dataframes[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "d477b745-9c6b-4fda-a8cf-6d4101e49e4d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 235,
     "status": "ok",
     "timestamp": 1721731928492,
     "user": {
      "displayName": "Inna Filatova",
      "userId": "01488652953098871073"
     },
     "user_tz": -120
    },
    "id": "d477b745-9c6b-4fda-a8cf-6d4101e49e4d",
    "outputId": "69cb60f0-325b-47ff-cf63-7e2c12b28b5d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mira aquí '"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Comando para quitar links\n",
    "\n",
    "def quitar_links(texto):\n",
    "    regex= r\"http\\S+\"\n",
    "    texto = re.sub(regex,\"\",texto)\n",
    "    return texto\n",
    "\n",
    "#Ejemplo quitar links\n",
    "\n",
    "quitar_links (\"Mira aquí https://www.zalando.es/mujer-home/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "3T_ltKsW3Z5M",
   "metadata": {
    "id": "3T_ltKsW3Z5M"
   },
   "outputs": [],
   "source": [
    "#Quitar links en reseñas filtradas\n",
    "\n",
    "filtered_df['body'] = filtered_df['body'].apply(quitar_links)\n",
    "for i in range (3):\n",
    "    dataframes[i]['body']= dataframes[i]['body'].apply(quitar_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "sj0HE9ki3fml",
   "metadata": {
    "id": "sj0HE9ki3fml"
   },
   "outputs": [],
   "source": [
    "#Resultado\n",
    "dataframes[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "vJOAxF1LEwg-",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 251,
     "status": "ok",
     "timestamp": 1721739950702,
     "user": {
      "displayName": "Inna Filatova",
      "userId": "01488652953098871073"
     },
     "user_tz": -120
    },
    "id": "vJOAxF1LEwg-",
    "outputId": "eb095662-d6f3-4ba8-98c7-608fb6b667f4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' que no sé  que no me gusta, po que nada, tambien he comrado una, tambien a mi hermana'"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Comando para quitar unas abreviaciones\n",
    "\n",
    "def quitar_abr(texto):\n",
    "    ques = r\"\\sq\\s|\\sqe\\s|\\sk\\s|\\Sq\\s|\\Sqe\\s|\\Sk\\s\" #variaciones de que\n",
    "    tbs = r\"\\stbn|\\stb|\\Stbn|\\Stb\" #variaciones de también\n",
    "    porques = r\"xqe\\s|pqe\\s|porq\\s|xq\\s|pq\\s|Xqe\\s|Pqe\\s|Porq\\s|Xq\\s|Pq\\s\" #variaciones de porque\n",
    "\n",
    "    texto_sin_abr = re.sub(ques, ' que ', texto)\n",
    "    texto_sin_abr = re.sub(tbs, ' tambien', texto_sin_abr)\n",
    "    texto_sin_abr = re.sub(porques, ' porque ', texto_sin_abr)\n",
    "    return texto_sin_abr\n",
    "\n",
    "#Ejemplo de quitar abr\n",
    "\n",
    "quitar_abr(\"Xq no sé pq no me gusta, porq nada, tbn he comrado una, tb a mi hermana\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "MnJLRwD43ii3",
   "metadata": {
    "executionInfo": {
     "elapsed": 235,
     "status": "ok",
     "timestamp": 1721742821541,
     "user": {
      "displayName": "Inna Filatova",
      "userId": "01488652953098871073"
     },
     "user_tz": -120
    },
    "id": "MnJLRwD43ii3"
   },
   "outputs": [],
   "source": [
    "#Quitar abreviacioes en reseñas filtradas\n",
    "\n",
    "filtered_df['body'] = filtered_df['body'].apply(quitar_abr)\n",
    "for i in range (3):\n",
    "    dataframes[i]['body']= dataframes[i]['body'].apply(quitar_abr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "m2GkM8ZY3n2s",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 313,
     "status": "ok",
     "timestamp": 1721742824990,
     "user": {
      "displayName": "Inna Filatova",
      "userId": "01488652953098871073"
     },
     "user_tz": -120
    },
    "id": "m2GkM8ZY3n2s",
    "outputId": "ee34bfe5-730a-4eb7-bd1d-7f408afda42a"
   },
   "outputs": [],
   "source": [
    "#Resultado\n",
    "dataframes[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "4f0927e4-b26a-4f51-afab-23b242c74e7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", sale frío .\n",
      "gustó . tarda calentar .\n",
      "excelente . delicioso fácil .\n"
     ]
    }
   ],
   "source": [
    "#Eliminar stop-words\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('es_core_news_md')\n",
    "stop_words_definidos = {\"cafetera\", \"cafeteras\", \"café\", \"producto\", \"máquina\", \"aparato\"}\n",
    "for word in stop_words_definidos:\n",
    "    nlp.vocab[word].is_stop = True\n",
    "\n",
    "def quitar_stopwords(texto):\n",
    "    doc = nlp(texto.lower())\n",
    "    palabras_filtradas = [token.text for token in doc if not token.is_stop]\n",
    "    return ' '.join(palabras_filtradas)\n",
    "\n",
    "#Ejemplo \n",
    "reseñas = [ \"La cafetera es muy buena, pero el café sale un poco frío.\",\n",
    "    \"Este producto no me gustó. La cafetera tarda mucho en calentar.\",\n",
    "    \"Excelente cafetera. Hace un café delicioso y es fácil de usar.\"]\n",
    "\n",
    "reseñas_limpias = [quitar_stopwords(reseña) for reseña in reseñas]\n",
    "for reseña in reseñas_limpias:\n",
    "    print(reseña)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "a620f054-aef9-4b91-b400-0520950af9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Quitar stopwords en las reseñas filtradas \n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('es_core_news_md')\n",
    "stop_words_definidos = {\"cafetera\", \"cafeteras\", \"café\", \"producto\", \"máquina\", \"aparato\"}\n",
    "for word in stop_words_definidos:\n",
    "    nlp.vocab[word].is_stop = True\n",
    "\n",
    "def quitar_stopwords(texto):\n",
    "    doc = nlp(texto.lower())\n",
    "    palabras_filtradas = [token.text for token in doc if not token.is_stop]\n",
    "    return ' '.join(palabras_filtradas)\n",
    "\n",
    "filtered_df['body'] = filtered_df['body'].apply(quitar_stopwords)\n",
    "for i in range(3):\n",
    "    dataframes[i]['body'] = dataframes[i]['body'].apply(quitar_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "QUfjXl2ovPfs",
   "metadata": {
    "executionInfo": {
     "elapsed": 242,
     "status": "ok",
     "timestamp": 1721740778881,
     "user": {
      "displayName": "Inna Filatova",
      "userId": "01488652953098871073"
     },
     "user_tz": -120
    },
    "id": "QUfjXl2ovPfs"
   },
   "outputs": [],
   "source": [
    "#Guardar los resultados en CSVs nuevos\n",
    "\n",
    "for i in range (3):\n",
    "    dataframes[i].to_csv(f\"rewiews_cleaned_{i}.csv\", index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "AFi5ZF3z6PTO",
   "metadata": {
    "id": "AFi5ZF3z6PTO"
   },
   "source": [
    "# Análisis de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "7812f79f-17b6-431c-8b58-f8b7ae6ca951",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aplicar lematización y tokenización a los CSVs limpiados \n",
    "\n",
    "import spacy\n",
    "import pandas as pd\n",
    "nlp = spacy.load(\"es_core_news_md\")\n",
    "def lematizar_tokenizar(texto):\n",
    "    doc= nlp(texto)\n",
    "    lemmas = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct]\n",
    "    tokens = [token.text for token in doc if not token.is_stop and not token.is_punct]\n",
    "    tokens_joined = ' '.join(tokens)\n",
    "    lemmas_joined = ' '.join(lemmas)\n",
    "    return tokens_joined, lemmas_joined\n",
    "\n",
    "# Lista con los nombres de los archivos CSV\n",
    "archivos_csv = ['rewiews_cleaned_0.csv', 'rewiews_cleaned_1.csv', 'rewiews_cleaned_2.csv']\n",
    "\n",
    "# Procesar cada archivo CSV\n",
    "df_mass=[]\n",
    "for archivo in archivos_csv:\n",
    "    # Leer el archivo CSV\n",
    "    df = pd.read_csv(archivo)\n",
    "    \n",
    "    # Aplicar la lematización y tokenización a la columna 'body'\n",
    "    df['tokens_lemmas'] = df['body'].astype(str).apply(lematizar_tokenizar)\n",
    "    df_mass.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "71e4bdf2-bf2a-4bd5-9d72-e24894f32a0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>rating</th>\n",
       "      <th>body</th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>tokens_lemmas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>No ha durado ni la garantía + pésimo servicio ...</td>\n",
       "      <td>1,0 de 5 estrellas</td>\n",
       "      <td>meses garantia durado   enviarla create estr...</td>\n",
       "      <td>MaX</td>\n",
       "      <td>Revisado en España el 3 de abril de 2024</td>\n",
       "      <td>(   meses garantia durado    enviarla create e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Está bien</td>\n",
       "      <td>3,0 de 5 estrellas</td>\n",
       "      <td>meses usandola cafe   materiales malos plastic...</td>\n",
       "      <td>Lorena</td>\n",
       "      <td>Revisado en España el 14 de marzo de 2024</td>\n",
       "      <td>(meses usandola cafe    materiales malos plast...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Perfecta!</td>\n",
       "      <td>5,0 de 5 estrellas</td>\n",
       "      <td>encanta   adicta cafe gusta tomarme espresso e...</td>\n",
       "      <td>Perfecta!</td>\n",
       "      <td>Revisado en España el 22 de febrero de 2024</td>\n",
       "      <td>(encanta    adicta cafe gusta tomarme espresso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EXCELENTE</td>\n",
       "      <td>5,0 de 5 estrellas</td>\n",
       "      <td>unica pega limpiar palito vapor calientaleche ...</td>\n",
       "      <td>Sil at</td>\n",
       "      <td>Revisado en España el 20 de febrero de 2024</td>\n",
       "      <td>(unica pega limpiar palito vapor calientaleche...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Un valle realmente bueno por el precio relativ...</td>\n",
       "      <td>5,0 de 5 estrellas</td>\n",
       "      <td>valle realmente precio relativamente</td>\n",
       "      <td>Sylvie O'Connell</td>\n",
       "      <td>Revisado en España el 5 de febrero de 2024</td>\n",
       "      <td>(valle realmente precio relativamente, valle r...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title              rating  \\\n",
       "0  No ha durado ni la garantía + pésimo servicio ...  1,0 de 5 estrellas   \n",
       "1                                          Está bien  3,0 de 5 estrellas   \n",
       "2                                          Perfecta!  5,0 de 5 estrellas   \n",
       "3                                          EXCELENTE  5,0 de 5 estrellas   \n",
       "4  Un valle realmente bueno por el precio relativ...  5,0 de 5 estrellas   \n",
       "\n",
       "                                                body            author  \\\n",
       "0    meses garantia durado   enviarla create estr...               MaX   \n",
       "1  meses usandola cafe   materiales malos plastic...            Lorena   \n",
       "2  encanta   adicta cafe gusta tomarme espresso e...         Perfecta!   \n",
       "3  unica pega limpiar palito vapor calientaleche ...            Sil at   \n",
       "4               valle realmente precio relativamente  Sylvie O'Connell   \n",
       "\n",
       "                                          date  \\\n",
       "0     Revisado en España el 3 de abril de 2024   \n",
       "1    Revisado en España el 14 de marzo de 2024   \n",
       "2  Revisado en España el 22 de febrero de 2024   \n",
       "3  Revisado en España el 20 de febrero de 2024   \n",
       "4   Revisado en España el 5 de febrero de 2024   \n",
       "\n",
       "                                       tokens_lemmas  \n",
       "0  (   meses garantia durado    enviarla create e...  \n",
       "1  (meses usandola cafe    materiales malos plast...  \n",
       "2  (encanta    adicta cafe gusta tomarme espresso...  \n",
       "3  (unica pega limpiar palito vapor calientaleche...  \n",
       "4  (valle realmente precio relativamente, valle r...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>rating</th>\n",
       "      <th>body</th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>tokens_lemmas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Recomendada!</td>\n",
       "      <td>5,0 de 5 estrellas</td>\n",
       "      <td>justo esperado   maquina cafe   servicio entre...</td>\n",
       "      <td>Cliente Amazon</td>\n",
       "      <td>Revisado en España el 8 de agosto de 2024</td>\n",
       "      <td>(justo esperado    maquina cafe    servicio en...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Buen producto</td>\n",
       "      <td>4,0 de 5 estrellas</td>\n",
       "      <td>funcion salvedad deposito cafe molido   sale a...</td>\n",
       "      <td>Jmgutierrez</td>\n",
       "      <td>Revisado en España el 4 de agosto de 2024</td>\n",
       "      <td>(funcion salvedad deposito cafe molido    sale...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Muy bien la cafetera, pero la caja vino rota, ...</td>\n",
       "      <td>4,0 de 5 estrellas</td>\n",
       "      <td>semiautomatica   promete   debes colocar caf...</td>\n",
       "      <td>Alfredo López</td>\n",
       "      <td>Revisado en España el 24 de julio de 2024</td>\n",
       "      <td>(   semiautomatica    promete    debes colocar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>la atencion y ayuda a personas mayores como yo</td>\n",
       "      <td>4,0 de 5 estrellas</td>\n",
       "      <td>bais personas mayores dificil nueba tecnolojia</td>\n",
       "      <td>olga EMPEZAR CON LA COMPRE DE EYER HABER SI EL...</td>\n",
       "      <td>Revisado en España el 18 de julio de 2024</td>\n",
       "      <td>(bais personas mayores dificil nueba tecnoloji...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Café de mala calidad</td>\n",
       "      <td>1,0 de 5 estrellas</td>\n",
       "      <td>desperdicia</td>\n",
       "      <td>Juanjo Guil</td>\n",
       "      <td>Revisado en España el 26 de junio de 2024</td>\n",
       "      <td>(desperdicia, desperdicia)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title              rating  \\\n",
       "0                                       Recomendada!  5,0 de 5 estrellas   \n",
       "1                                      Buen producto  4,0 de 5 estrellas   \n",
       "2  Muy bien la cafetera, pero la caja vino rota, ...  4,0 de 5 estrellas   \n",
       "3     la atencion y ayuda a personas mayores como yo  4,0 de 5 estrellas   \n",
       "4                               Café de mala calidad  1,0 de 5 estrellas   \n",
       "\n",
       "                                                body  \\\n",
       "0  justo esperado   maquina cafe   servicio entre...   \n",
       "1  funcion salvedad deposito cafe molido   sale a...   \n",
       "2    semiautomatica   promete   debes colocar caf...   \n",
       "3     bais personas mayores dificil nueba tecnolojia   \n",
       "4                                        desperdicia   \n",
       "\n",
       "                                              author  \\\n",
       "0                                     Cliente Amazon   \n",
       "1                                        Jmgutierrez   \n",
       "2                                      Alfredo López   \n",
       "3  olga EMPEZAR CON LA COMPRE DE EYER HABER SI EL...   \n",
       "4                                        Juanjo Guil   \n",
       "\n",
       "                                        date  \\\n",
       "0  Revisado en España el 8 de agosto de 2024   \n",
       "1  Revisado en España el 4 de agosto de 2024   \n",
       "2  Revisado en España el 24 de julio de 2024   \n",
       "3  Revisado en España el 18 de julio de 2024   \n",
       "4  Revisado en España el 26 de junio de 2024   \n",
       "\n",
       "                                       tokens_lemmas  \n",
       "0  (justo esperado    maquina cafe    servicio en...  \n",
       "1  (funcion salvedad deposito cafe molido    sale...  \n",
       "2  (   semiautomatica    promete    debes colocar...  \n",
       "3  (bais personas mayores dificil nueba tecnoloji...  \n",
       "4                         (desperdicia, desperdicia)  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>rating</th>\n",
       "      <th>body</th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>tokens_lemmas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>El Vendedor Markenet se limpia las manos</td>\n",
       "      <td>1,0 de 5 estrellas</td>\n",
       "      <td>hola   principio llego defectuosa   cable alim...</td>\n",
       "      <td>Mercedes</td>\n",
       "      <td>Revisado en España el 13 de marzo de 2024</td>\n",
       "      <td>(hola    principio llego defectuosa    cable a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fatal</td>\n",
       "      <td>1,0 de 5 estrellas</td>\n",
       "      <td>vendieron cafeters top \\n cafe aguado   \\n rec...</td>\n",
       "      <td>RAQUEL</td>\n",
       "      <td>Revisado en España el 6 de febrero de 2024</td>\n",
       "      <td>(vendieron cafeters top \\n  cafe aguado   \\n  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cápsulas muy ricas pero muy caras</td>\n",
       "      <td>1,0 de 5 estrellas</td>\n",
       "      <td>deposito agua duro rompe facilidad tienes enca...</td>\n",
       "      <td>maria</td>\n",
       "      <td>Revisado en España el 21 de enero de 2023</td>\n",
       "      <td>(deposito agua duro rompe facilidad tienes enc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Muy buena</td>\n",
       "      <td>5,0 de 5 estrellas</td>\n",
       "      <td>bonita     pequeña     cafe excelente importa ...</td>\n",
       "      <td>maria giovanna</td>\n",
       "      <td>Revisado en España el 19 de noviembre de 2019</td>\n",
       "      <td>(bonita      pequeña      cafe excelente impor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>He recibido la cafetera “LAVAZZA. A MODO MÍO” ...</td>\n",
       "      <td>1,0 de 5 estrellas</td>\n",
       "      <td>usarla encuentro extraordinariamente pequeña  ...</td>\n",
       "      <td>Isidoro</td>\n",
       "      <td>Revisado en España el 13 de noviembre de 2019</td>\n",
       "      <td>(usarla encuentro extraordinariamente pequeña ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title              rating  \\\n",
       "0           El Vendedor Markenet se limpia las manos  1,0 de 5 estrellas   \n",
       "1                                              Fatal  1,0 de 5 estrellas   \n",
       "2                  Cápsulas muy ricas pero muy caras  1,0 de 5 estrellas   \n",
       "3                                          Muy buena  5,0 de 5 estrellas   \n",
       "4  He recibido la cafetera “LAVAZZA. A MODO MÍO” ...  1,0 de 5 estrellas   \n",
       "\n",
       "                                                body          author  \\\n",
       "0  hola   principio llego defectuosa   cable alim...        Mercedes   \n",
       "1  vendieron cafeters top \\n cafe aguado   \\n rec...          RAQUEL   \n",
       "2  deposito agua duro rompe facilidad tienes enca...           maria   \n",
       "3  bonita     pequeña     cafe excelente importa ...  maria giovanna   \n",
       "4  usarla encuentro extraordinariamente pequeña  ...         Isidoro   \n",
       "\n",
       "                                            date  \\\n",
       "0      Revisado en España el 13 de marzo de 2024   \n",
       "1     Revisado en España el 6 de febrero de 2024   \n",
       "2      Revisado en España el 21 de enero de 2023   \n",
       "3  Revisado en España el 19 de noviembre de 2019   \n",
       "4  Revisado en España el 13 de noviembre de 2019   \n",
       "\n",
       "                                       tokens_lemmas  \n",
       "0  (hola    principio llego defectuosa    cable a...  \n",
       "1  (vendieron cafeters top \\n  cafe aguado   \\n  ...  \n",
       "2  (deposito agua duro rompe facilidad tienes enc...  \n",
       "3  (bonita      pequeña      cafe excelente impor...  \n",
       "4  (usarla encuentro extraordinariamente pequeña ...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#Resultado\n",
    "\n",
    "for i in range(3):\n",
    "    display(df_mass[i].head())\n",
    "    print(\"-\"*50)\n",
    "\n",
    "#Guardar resultado en CSV\n",
    "\n",
    "for i in range (3):\n",
    "    df_mass[i].to_csv(f\"rewiews_cleaned_tokenized_{i}.csv\", index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "c14ee733-946d-47cd-80dd-988a1b99b4d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "/Applications/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "/Applications/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Análisis de sentimiento mediante Pysentimiento \n",
    "\n",
    "import pandas as pd\n",
    "from pysentimiento import create_analyzer\n",
    "\n",
    "reseñas=[]\n",
    "frecuencias=[]\n",
    "\n",
    "for i in range(3):\n",
    "    reseñas.append(pd.read_csv(f\"rewiews_cleaned_tokenized_{i}.csv\"))\n",
    "    analyzer = create_analyzer(task=\"sentiment\", lang=\"es\")\n",
    "    reseñas[i]['emociones'] = reseñas[i]['tokens_lemmas'].apply(lambda x: analyzer.predict(x).output)\n",
    "    frecuencias.append(reseñas[i]['emociones'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "6303b22a-4742-4d91-b4d7-2cdeec366196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[emociones\n",
      "NEU    15322\n",
      "POS     2813\n",
      "NEG     2059\n",
      "Name: count, dtype: int64, emociones\n",
      "NEU    11565\n",
      "NEG     2410\n",
      "POS     1958\n",
      "Name: count, dtype: int64, emociones\n",
      "NEU    14568\n",
      "POS     2338\n",
      "NEG     1738\n",
      "Name: count, dtype: int64]\n"
     ]
    }
   ],
   "source": [
    "print(frecuencias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "f12db758-6e45-492b-8973-a3742cf5fa9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "/Applications/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "/Applications/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Análisis de sentimiento mediante Pysentimiento (sin tokenizar)\n",
    "\n",
    "import pandas as pd\n",
    "from pysentimiento import create_analyzer\n",
    "\n",
    "reseñas=[]\n",
    "frecuencias=[]\n",
    "\n",
    "for i in range(3):\n",
    "    reseñas.append(pd.read_csv(f\"rewiews_cleaned_{i}.csv\"))\n",
    "    analyzer = create_analyzer(task=\"sentiment\", lang=\"es\")\n",
    "    reseñas[i]['emociones'] = reseñas[i]['body'].astype(str).apply(lambda x: analyzer.predict(x).output)\n",
    "    frecuencias.append(reseñas[i]['emociones'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "b09d99c4-5730-480a-9585-a874369f3c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nuevo_archivo = f\"rewiews_analyzed_Py_sentimiento{i}.csv\"\n",
    "df.to_csv(nuevo_archivo, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "eb3cca57-68d0-452b-9d96-c4323a9ce56d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[emociones\n",
      "NEU    11256\n",
      "POS     5744\n",
      "NEG     3194\n",
      "Name: count, dtype: int64, emociones\n",
      "NEU    8795\n",
      "POS    3646\n",
      "NEG    3492\n",
      "Name: count, dtype: int64, emociones\n",
      "NEU    11067\n",
      "POS     4646\n",
      "NEG     2931\n",
      "Name: count, dtype: int64]\n"
     ]
    }
   ],
   "source": [
    "print(frecuencias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "3aec0d3c-d551-426a-94fb-64aa66724c87",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'category'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.12/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'category'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[351], line 8\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      6\u001b[0m categories \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArchivo \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m3\u001b[39m)]\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m base[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategory\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m      9\u001b[0m     categories\u001b[38;5;241m.\u001b[39mappend(i)\n\u001b[1;32m     11\u001b[0m labels \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPositive\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNeutral\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNegative\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.12/site-packages/pandas/core/frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[1;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.12/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'category'"
     ]
    }
   ],
   "source": [
    "#Crear diagramas para resultados Pysentimiento (análisis de sentimiento)\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "reseñas = []\n",
    "frecuencias = []\n",
    "categories = []\n",
    "\n",
    "for i in base[\"category\"]:\n",
    "    categories.append(i)\n",
    "\n",
    "labels = ['Positive', 'Neutral', 'Negative']\n",
    "\n",
    "\n",
    "fig, axis = plt.subplots(1, 3, figsize=(18, 6)) # creating a shape and an axis\n",
    "\n",
    "for i, (ax, data) in enumerate(zip(axis, polarities)):\n",
    "    ax.pie(data, labels=labels, autopct='%1.1f%%', colors=['#66b3ff','#ffcc99','#ff6666'])\n",
    "    ax.set_title(categories[i])\n",
    "\n",
    "plt.tight_layout() #showing diagrams\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "128c5ef0-8f72-4172-95bb-4e6a0c0458eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[357], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m reseñas\u001b[38;5;241m.\u001b[39mappend(pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrewiews_cleaned_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Aplicar el análisis de emociones y almacenar los resultados en una nueva columna\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m reseñas[i][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124memociones\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m reseñas[i][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbody\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: emotion_analyzer\u001b[38;5;241m.\u001b[39mpredict(x)\u001b[38;5;241m.\u001b[39moutput)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Calcular las frecuencias de cada emoción y almacenarlas\u001b[39;00m\n\u001b[1;32m     20\u001b[0m frecuencias\u001b[38;5;241m.\u001b[39mappend(reseñas[i][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124memociones\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalue_counts())\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.12/site-packages/pandas/core/series.py:4924\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4800\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SeriesApply(\n\u001b[1;32m   4918\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4919\u001b[0m         func,\n\u001b[1;32m   4920\u001b[0m         convert_dtype\u001b[38;5;241m=\u001b[39mconvert_dtype,\n\u001b[1;32m   4921\u001b[0m         by_row\u001b[38;5;241m=\u001b[39mby_row,\n\u001b[1;32m   4922\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[1;32m   4923\u001b[0m         kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[0;32m-> 4924\u001b[0m     )\u001b[38;5;241m.\u001b[39mapply()\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.12/site-packages/pandas/core/apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[1;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[0;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_standard()\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.12/site-packages/pandas/core/apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[1;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[1;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[1;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_map_values(\n\u001b[1;32m   1508\u001b[0m     mapper\u001b[38;5;241m=\u001b[39mcurried, na_action\u001b[38;5;241m=\u001b[39maction, convert\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_dtype\n\u001b[1;32m   1509\u001b[0m )\n\u001b[1;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.12/site-packages/pandas/core/base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[1;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[0;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m algorithms\u001b[38;5;241m.\u001b[39mmap_array(arr, mapper, na_action\u001b[38;5;241m=\u001b[39mna_action, convert\u001b[38;5;241m=\u001b[39mconvert)\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.12/site-packages/pandas/core/algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer(values, mapper, convert\u001b[38;5;241m=\u001b[39mconvert)\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[1;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[1;32m   1747\u001b[0m     )\n",
      "File \u001b[0;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn[357], line 17\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     14\u001b[0m reseñas\u001b[38;5;241m.\u001b[39mappend(pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrewiews_cleaned_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Aplicar el análisis de emociones y almacenar los resultados en una nueva columna\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m reseñas[i][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124memociones\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m reseñas[i][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbody\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: emotion_analyzer\u001b[38;5;241m.\u001b[39mpredict(x)\u001b[38;5;241m.\u001b[39moutput)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Calcular las frecuencias de cada emoción y almacenarlas\u001b[39;00m\n\u001b[1;32m     20\u001b[0m frecuencias\u001b[38;5;241m.\u001b[39mappend(reseñas[i][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124memociones\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalue_counts())\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.12/site-packages/pysentimiento/analyzer.py:334\u001b[0m, in \u001b[0;36mAnalyzerForSequenceClassification.predict\u001b[0;34m(self, inputs, context, target, preprocess_context)\u001b[0m\n\u001b[1;32m    332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m context \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(context, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    333\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContext must be a string\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 334\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predict_single(\n\u001b[1;32m    335\u001b[0m         inputs, context\u001b[38;5;241m=\u001b[39mcontext, preprocess_context\u001b[38;5;241m=\u001b[39mpreprocess_context\n\u001b[1;32m    336\u001b[0m     )\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m context \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(context, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContext must be a list of strings\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.12/site-packages/pysentimiento/analyzer.py:301\u001b[0m, in \u001b[0;36mAnalyzerForSequenceClassification._predict_single\u001b[0;34m(self, sentence, context, preprocess_context)\u001b[0m\n\u001b[1;32m    289\u001b[0m     inputs\u001b[38;5;241m.\u001b[39mappend(context)\n\u001b[1;32m    290\u001b[0m idx \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    291\u001b[0m     torch\u001b[38;5;241m.\u001b[39mLongTensor(\n\u001b[1;32m    292\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mencode(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    300\u001b[0m )\n\u001b[0;32m--> 301\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(idx)\n\u001b[1;32m    302\u001b[0m logits \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mlogits\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_output(sentence, logits)\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.12/site-packages/transformers/models/roberta/modeling_roberta.py:1195\u001b[0m, in \u001b[0;36mRobertaForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1189\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1193\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1195\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroberta(\n\u001b[1;32m   1196\u001b[0m     input_ids,\n\u001b[1;32m   1197\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m   1198\u001b[0m     token_type_ids\u001b[38;5;241m=\u001b[39mtoken_type_ids,\n\u001b[1;32m   1199\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m   1200\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[1;32m   1201\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[1;32m   1202\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m   1203\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[1;32m   1204\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m   1205\u001b[0m )\n\u001b[1;32m   1206\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1207\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(sequence_output)\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.12/site-packages/transformers/models/roberta/modeling_roberta.py:832\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    823\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m    825\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m    826\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m    827\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    830\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[1;32m    831\u001b[0m )\n\u001b[0;32m--> 832\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\n\u001b[1;32m    833\u001b[0m     embedding_output,\n\u001b[1;32m    834\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mextended_attention_mask,\n\u001b[1;32m    835\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[1;32m    836\u001b[0m     encoder_hidden_states\u001b[38;5;241m=\u001b[39mencoder_hidden_states,\n\u001b[1;32m    837\u001b[0m     encoder_attention_mask\u001b[38;5;241m=\u001b[39mencoder_extended_attention_mask,\n\u001b[1;32m    838\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[1;32m    839\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[1;32m    840\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m    841\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[1;32m    842\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m    843\u001b[0m )\n\u001b[1;32m    844\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    845\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.12/site-packages/transformers/models/roberta/modeling_roberta.py:521\u001b[0m, in \u001b[0;36mRobertaEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    510\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    511\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    512\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    518\u001b[0m         output_attentions,\n\u001b[1;32m    519\u001b[0m     )\n\u001b[1;32m    520\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 521\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m layer_module(\n\u001b[1;32m    522\u001b[0m         hidden_states,\n\u001b[1;32m    523\u001b[0m         attention_mask,\n\u001b[1;32m    524\u001b[0m         layer_head_mask,\n\u001b[1;32m    525\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    526\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    527\u001b[0m         past_key_value,\n\u001b[1;32m    528\u001b[0m         output_attentions,\n\u001b[1;32m    529\u001b[0m     )\n\u001b[1;32m    531\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    532\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.12/site-packages/transformers/models/roberta/modeling_roberta.py:410\u001b[0m, in \u001b[0;36mRobertaLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    399\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    400\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    407\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m    408\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    409\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 410\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention(\n\u001b[1;32m    411\u001b[0m         hidden_states,\n\u001b[1;32m    412\u001b[0m         attention_mask,\n\u001b[1;32m    413\u001b[0m         head_mask,\n\u001b[1;32m    414\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m    415\u001b[0m         past_key_value\u001b[38;5;241m=\u001b[39mself_attn_past_key_value,\n\u001b[1;32m    416\u001b[0m     )\n\u001b[1;32m    417\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    419\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.12/site-packages/transformers/models/roberta/modeling_roberta.py:337\u001b[0m, in \u001b[0;36mRobertaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    328\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    329\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    335\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    336\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m--> 337\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself(\n\u001b[1;32m    338\u001b[0m         hidden_states,\n\u001b[1;32m    339\u001b[0m         attention_mask,\n\u001b[1;32m    340\u001b[0m         head_mask,\n\u001b[1;32m    341\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    342\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    343\u001b[0m         past_key_value,\n\u001b[1;32m    344\u001b[0m         output_attentions,\n\u001b[1;32m    345\u001b[0m     )\n\u001b[1;32m    346\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[1;32m    347\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.12/site-packages/transformers/models/roberta/modeling_roberta.py:266\u001b[0m, in \u001b[0;36mRobertaSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m head_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    264\u001b[0m     attention_probs \u001b[38;5;241m=\u001b[39m attention_probs \u001b[38;5;241m*\u001b[39m head_mask\n\u001b[0;32m--> 266\u001b[0m context_layer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(attention_probs, value_layer)\n\u001b[1;32m    268\u001b[0m context_layer \u001b[38;5;241m=\u001b[39m context_layer\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[1;32m    269\u001b[0m new_context_layer_shape \u001b[38;5;241m=\u001b[39m context_layer\u001b[38;5;241m.\u001b[39msize()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m+\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_head_size,)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Analisis de emociones con Pysentiment\n",
    "\n",
    "import pandas as pd\n",
    "from pysentimiento import create_analyzer\n",
    "\n",
    "emotion_analyzer = create_analyzer(task=\"emotion\", lang=\"es\")\n",
    "\n",
    "reseñas = []\n",
    "frecuencias = []\n",
    "\n",
    "for i in range(3):\n",
    "    reseñas.append(pd.read_csv(f\"rewiews_cleaned_{i}.csv\"))\n",
    "    \n",
    "    reseñas[i]['emociones'] = reseñas[i]['body'].astype(str).apply(lambda x: emotion_analyzer.predict(x).output)\n",
    "    frecuencias.append(reseñas[i]['emociones'].value_counts())  # Calcular las frecuencias de cada emoción y almacenarlas\n",
    "\n",
    "# Mostrar los resultados de las frecuencias para cada dataset\n",
    "for i in range(3):\n",
    "    print(f\"Frecuencias de emociones para Archivo {i}:\")\n",
    "    print(frecuencias[i])\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f8ddfb-b547-48b7-a4b3-1efd1cbf232c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Analisis SpaCy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb6c108-40a0-47f6-91c7-293536a213e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Análisis de sentimiento con SpaCy (TextBlob)\n",
    "\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from textblob import TextBlob\n",
    "from tqdm import tqdm\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def analyze_sentiment(review):\n",
    "\n",
    "    doc = nlp(review) # tokenizer \n",
    "    \n",
    "    processed_review = \" \".join([token.text for token in doc]) # adjusting for TextBlob\n",
    "\n",
    "    blob = TextBlob(processed_review)\n",
    "    \n",
    "    polarity = blob.sentiment.polarity # getting polarity between -1.0 and 1.0\n",
    "\n",
    "    if polarity > 0:\n",
    "        return 'POS' \n",
    "    if polarity < 0:\n",
    "        return 'NEG'\n",
    "    else:\n",
    "        return 'NEU'\n",
    "\n",
    "if name == \"__main__\":\n",
    "\n",
    "    df_TextBlob = []\n",
    "    for i in range(3):\n",
    "        df = pd.read_csv(f'rewiews_cleaned{i}.csv')\n",
    "\n",
    "        for i in tqdm(range(df.shape[0]), desc = f\"Processing reviews in category_{i}.csv\"):\n",
    "    \n",
    "            polarity = analyze_sentiment(df.loc[i]['body'])\n",
    "            df['TextBlob_sentiment'] = polarity\n",
    "\n",
    "        df_TextBlob.append(df)\n",
    "\n",
    "\n",
    "for i in range(3):\n",
    "    df_TextBlob[i].to_csv(f'rewiews_cleaned/analyzed_category_{i}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "7d353371-97e0-45bb-9836-149ba02fabe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Crear diagramas para resultados SpaCy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "base = pd.read_csv(\"new_database/base.csv\") # naming categories\n",
    "categories = []\n",
    "\n",
    "for i in base[\"category\"]:\n",
    "    categories.append(i)\n",
    "\n",
    "labels = ['Positive', 'Neutral', 'Negative']\n",
    "\n",
    "\n",
    "fig, axis = plt.subplots(1, 3, figsize=(18, 6)) # creating a shape and an axis\n",
    "\n",
    "for i, (ax, data) in enumerate(zip(axis, polarities)):\n",
    "    ax.pie(data, labels=labels, autopct='%1.1f%%', colors=['#66b3ff','#ffcc99','#ff6666'])\n",
    "    ax.set_title(categories[i])\n",
    "\n",
    "plt.tight_layout() #showing diagrams\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3e792b8f-7c3f-4978-bf5b-d62f90d90184",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>rating</th>\n",
       "      <th>body</th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>emociones</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Regalo</td>\n",
       "      <td>5,0 de 5 estrellas</td>\n",
       "      <td>Me gustó mucho</td>\n",
       "      <td>Cliente Amazon</td>\n",
       "      <td>Revisado en España el 6 de abril de 2024</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>No ha durado ni la garantía + pésimo servicio ...</td>\n",
       "      <td>1,0 de 5 estrellas</td>\n",
       "      <td>Ni los 24 meses de garantía ha durado, y al en...</td>\n",
       "      <td>MaX</td>\n",
       "      <td>Revisado en España el 3 de abril de 2024</td>\n",
       "      <td>NEG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Está bien</td>\n",
       "      <td>3,0 de 5 estrellas</td>\n",
       "      <td>Después de más de dos meses usándola hace muy ...</td>\n",
       "      <td>Lorena</td>\n",
       "      <td>Revisado en España el 14 de marzo de 2024</td>\n",
       "      <td>NEG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Perfecta!</td>\n",
       "      <td>5,0 de 5 estrellas</td>\n",
       "      <td>Me encanta! Soy adicta al café y me gusta toma...</td>\n",
       "      <td>Perfecta!</td>\n",
       "      <td>Revisado en España el 22 de febrero de 2024</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EXCELENTE</td>\n",
       "      <td>5,0 de 5 estrellas</td>\n",
       "      <td>como única pega que limpiar el palito del vapo...</td>\n",
       "      <td>Sil at</td>\n",
       "      <td>Revisado en España el 20 de febrero de 2024</td>\n",
       "      <td>NEG</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title              rating  \\\n",
       "0                                             Regalo  5,0 de 5 estrellas   \n",
       "1  No ha durado ni la garantía + pésimo servicio ...  1,0 de 5 estrellas   \n",
       "2                                          Está bien  3,0 de 5 estrellas   \n",
       "3                                          Perfecta!  5,0 de 5 estrellas   \n",
       "4                                          EXCELENTE  5,0 de 5 estrellas   \n",
       "\n",
       "                                                body          author  \\\n",
       "0                                     Me gustó mucho  Cliente Amazon   \n",
       "1  Ni los 24 meses de garantía ha durado, y al en...             MaX   \n",
       "2  Después de más de dos meses usándola hace muy ...          Lorena   \n",
       "3  Me encanta! Soy adicta al café y me gusta toma...       Perfecta!   \n",
       "4  como única pega que limpiar el palito del vapo...          Sil at   \n",
       "\n",
       "                                          date emociones  \n",
       "0     Revisado en España el 6 de abril de 2024       POS  \n",
       "1     Revisado en España el 3 de abril de 2024       NEG  \n",
       "2    Revisado en España el 14 de marzo de 2024       NEG  \n",
       "3  Revisado en España el 22 de febrero de 2024       POS  \n",
       "4  Revisado en España el 20 de febrero de 2024       NEG  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reseñas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e9572a-e1e2-4527-9233-e07d9d1faf4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparar resultados SpaCy y Pysentimiento\n",
    "\n",
    "\n",
    "# Resultados de las clasificaciones\n",
    "sentimientos_spacy = ['']\n",
    "sentimientos_pysentimiento = ['] \n",
    "\n",
    "# Inicializar contadores\n",
    "consistentes = 0\n",
    "discrepantes = 0\n",
    "indices_discrepantes = []\n",
    "\n",
    "# Comparar las clasificaciones\n",
    "for i in range(len(sentimientos_spacy)):\n",
    "    if sentimientos_spacy[i] == sentimientos_pysentimiento[i]:\n",
    "        consistentes += 1\n",
    "    else:\n",
    "        discrepantes += 1\n",
    "        indices_discrepantes.append(i)\n",
    "\n",
    "# Mostrar resultados\n",
    "print(f\"Reseñas consistentes: {consistentes}\")\n",
    "print(f\"Reseñas discrepantes: {discrepantes}\")\n",
    "\n",
    "# Calcular porcentaje de consistencia\n",
    "total_reseñas = len(sentimientos_spacy)\n",
    "porcentaje_consistencia = (consistentes / total_reseñas) * 100\n",
    "print(f\"Porcentaje de reseñas consistentemente clasificadas: {porcentaje_consistencia:.2f}%\")\n",
    "\n",
    "# Ejemplo de lista de reseñas (texto real de las reseñas)\n",
    "reseñas = []\n",
    "\n",
    "# Mostrar reseñas discrepantes\n",
    "print(\"Reseñas discrepantes y sus clasificaciones:\")\n",
    "for i in indices_discrepantes:\n",
    "    print(f\"Reseña: {reseñas[i]}\")\n",
    "    print(f\"Clasificación spaCy: {sentimientos_spacy[i]}\")\n",
    "    print(f\"Clasificación pysentimiento: {sentimientos_pysentimiento[i]}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5e229e-cd01-47d3-92f2-deffc5ce795d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "AFi5ZF3z6PTO"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
