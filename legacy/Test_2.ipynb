{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<p style=\"text-align: center; font-size: 25px;\">\n",
    "    <strong>\n",
    "          Final attemt to collect data manually\n",
    "    </strong>\n",
    "</p>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def add_to_0csv(goods_url, reviews_url, filename=f'database/0.csv'):\n",
    "    if os.path.exists(filename):\n",
    "        df = pd.read_csv(filename)\n",
    "    else:\n",
    "        df = pd.DataFrame(columns=['ID', 'goods_url', 'reviews_url'])\n",
    "    \n",
    "    next_id = os.path.join('', f'0-{len(df):06}')\n",
    "    \n",
    "    new_row = {'ID': next_id, 'goods_url': goods_url, 'reviews_url': reviews_url}\n",
    "    \n",
    "    if not df[(df['goods_url'] == goods_url) & (df['reviews_url'] == reviews_url)].empty:\n",
    "        print(f\"Duplicate row found. The row {goods_url} will not be added.\")\n",
    "        return\n",
    "    \n",
    "    df = df.append(new_row, ignore_index=True)\n",
    "    \n",
    "    df.to_csv(filename, index=False, encoding='utf-8')\n",
    "    \n",
    "    print(f\"Row {goods_url} added successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'index = 0\\ntry:\\n    reviews_url = get_reviews_url_by_index(index)\\n    print(f\"The reviews_url at index {index} is: {reviews_url}\")\\nexcept IndexError as e:\\n    print(e)'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_reviews_url_by_index(index, filename):\n",
    "\n",
    "    df = pd.read_csv(filename)\n",
    "    \n",
    "    index = index.split('-')[1]\n",
    "    index = int(index)\n",
    "    \n",
    "    if index < 0 or index >= len(df):\n",
    "        raise IndexError(\"ID out of range of DataFrame\")\n",
    "    \n",
    "    reviews_url = df.loc[index, 'reviews_url']\n",
    "    \n",
    "    return reviews_url\n",
    "\n",
    "'''index = 0\n",
    "try:\n",
    "    reviews_url = get_reviews_url_by_index(index)\n",
    "    print(f\"The reviews_url at index {index} is: {reviews_url}\")\n",
    "except IndexError as e:\n",
    "    print(e)'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_to_0csv('https://www.amazon.es/Russell-Hobbs-24031-56-temporizador-programable/dp/B07G8JQSQR/ref=sr_1_7?dib=eyJ2IjoiMSJ9.IrJ3PxC-nG-YqhUNnIX9Jiw48ZDvNl0MF5wJiRGSjvyBvn3Ea4STk7gQzz_U5juaF9Gd55ePZJRnt0cywE-JQBEwH4ig2xjXmYyJSVhv2KA8ikTXZfRh9vbdueRv01Xn5qkyvx_v9oTfdOdDoBoO8XzsgPSvpSdsIUpr9VvdKMv3Xj9pVrLvBSjil-j0PGkVrjLpzplF1lhQ482pXDhTyruERgk36NxP9kKM6hiUtP94kI5vofPPR4WBmnknyDFBGbV3R1_N-NndQ9hIRv_4nSfZydEJgqpatQBOwEgcqiA.O0Qc8bGdd4ab0WFE_W89ciuftYartK44g_8E5n5G2lE&dib_tag=se&qid=1721654564&s=kitchen&sr=1-7', 'https://www.amazon.es/Russell-Hobbs-24031-56-temporizador-programable/product-reviews/B07G8JQSQR/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row https://www.amazon.es/Jocca-Cafetera-Italiana-El%C3%A9ctrica added successfully.\n"
     ]
    }
   ],
   "source": [
    "add_to_0csv('https://www.amazon.es/Jocca-Cafetera-Italiana-El%C3%A9ctrica', 'https://www.amazon.es/Jocca-Cafetera-Italiana-El%C3%A9ctrica-autom%C3%A1tico/product-reviews/B0CS3MCF9J/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate row found. The row https://www.amazon.es/dp/B08H585PJ2/ref=sspa_dk_detail_1?pd_rd_i=B08H585PJ2&pd_rd_w=bH8km&content-id=amzn1.sym.fa290e14-a761-4185-b21b-f2448830508b&pf_rd_p=fa290e14-a761-4185-b21b-f2448830508b&pf_rd_r=HAJNY0ATPX2NBZCXJQ14&pd_rd_wg=GzCC8&pd_rd_r=20bc4717-0293-4421-b315-9582bba3493f&sp_csd=d2lkZ2V0TmFtZT1zcF9kZXRhaWxfdGhlbWF0aWM&th=1 will not be added.\n"
     ]
    }
   ],
   "source": [
    "add_to_0csv('https://www.amazon.es/dp/B08H585PJ2/ref=sspa_dk_detail_1?pd_rd_i=B08H585PJ2&pd_rd_w=bH8km&content-id=amzn1.sym.fa290e14-a761-4185-b21b-f2448830508b&pf_rd_p=fa290e14-a761-4185-b21b-f2448830508b&pf_rd_r=HAJNY0ATPX2NBZCXJQ14&pd_rd_wg=GzCC8&pd_rd_r=20bc4717-0293-4421-b315-9582bba3493f&sp_csd=d2lkZ2V0TmFtZT1zcF9kZXRhaWxfdGhlbWF0aWM&th=1', 'https://www.amazon.es/Easyworkz-Eclipse-filtraci%C3%B3n-resistente-borosilicato/product-reviews/B08H585PJ2/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reviews_url at index 0-000278 is: https://www.amazon.es/Melitta-Cafetera-Filtro-Vidrio-Capacidad/product-reviews/B07GFPJ6YY/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing URLs: 100%|██████████| 80/80 [02:41<00:00,  2.02s/URL]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing is done.\n",
      "Saved 509 individual reviews in database/reviews/0-0-000278.csv\n",
      "------------------------------------------------------------------\n",
      "Time taken: 162.63 seconds\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_urls(ID, filename):\n",
    "    \n",
    "    try:\n",
    "        base_url = get_reviews_url_by_index(ID, filename)\n",
    "        print(f\"The reviews_url at index {ID} is: {base_url}\")\n",
    "    except IndexError as e:\n",
    "        print(e)\n",
    "    \n",
    "    filters = [\n",
    "        'sortBy=recent',\n",
    "        'sortBy=helpful',\n",
    "        'sortBy=rating',\n",
    "        'filterByStar=one_star',\n",
    "        'filterByStar=two_star',\n",
    "        'filterByStar=three_star',\n",
    "        'filterByStar=four_star',\n",
    "        'filterByStar=five_star'\n",
    "    ]\n",
    "    \n",
    "    list_urls = []\n",
    "    for filter_option in filters:\n",
    "        for page in range(1, 11):\n",
    "            \n",
    "            #updating the filter and page number \n",
    "            updated_url = f\"{base_url}&{filter_option}&pageNumber={page}\"\n",
    "            list_urls.append(updated_url)\n",
    "    \n",
    "    return list_urls\n",
    "\n",
    "def scrape_reviews(urls):\n",
    "    \n",
    "    options = Options()\n",
    "    options.add_argument('--headless')\n",
    "    options.add_argument('--no-sandbox')\n",
    "    options.add_argument('--disable-dev-shm-usage')\n",
    "    options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\")\n",
    "\n",
    "    DRIVER_PATH = '/Users/apple/Downloads/Project_Gnomi_Huekradi/chromedriver'\n",
    "\n",
    "    service = Service(DRIVER_PATH)\n",
    "    driver = webdriver.Chrome(service=service, options=options)\n",
    "    \n",
    "    reviews = []\n",
    "    \n",
    "    for url in tqdm(urls, desc='Processing URLs', unit='URL'): \n",
    "        driver.get(url)\n",
    "\n",
    "        time.sleep(1)\n",
    "\n",
    "        # Getting reviews\n",
    "        review_blocks = driver.find_elements(By.CSS_SELECTOR, '.a-section.review')\n",
    "        for review_block in review_blocks:\n",
    "            title = review_block.find_element(By.CSS_SELECTOR, '.review-title-content').text.strip()\n",
    "            rating = review_block.find_element(By.CSS_SELECTOR, '.a-icon-alt').get_attribute('textContent').strip()\n",
    "            body = review_block.find_element(By.CSS_SELECTOR, '[data-hook=\"review-body\"]').text.strip()\n",
    "            author = review_block.find_element(By.CSS_SELECTOR, '.a-profile-name').text.strip()\n",
    "            date = review_block.find_element(By.CSS_SELECTOR, '.review-date').text.strip()\n",
    "\n",
    "            reviews.append({\n",
    "                'title': title,\n",
    "                'rating': rating,\n",
    "                'body': body,\n",
    "                'author': author,\n",
    "                'date': date,\n",
    "            })\n",
    "    \n",
    "    print(\"Parsing is done.\")\n",
    "    driver.quit()\n",
    "    return reviews\n",
    "\n",
    "# Removing duplicates and saving data\n",
    "def save_to_csv(reviews, filename):\n",
    "    df = pd.DataFrame(reviews)\n",
    "    df.drop_duplicates(subset=['title', 'body', 'author'], inplace=True)\n",
    "    df.to_csv(filename, index=False, encoding='utf-8')\n",
    "    \n",
    "    return int(df.shape[0])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    start_time = time.perf_counter() \n",
    "    \n",
    "    #queue = [53, 54, 55, 56] # from 233 to 276\n",
    "    \n",
    "    queue = [278]\n",
    "    \n",
    "    #up to page 45\n",
    "    \n",
    "    for i in queue:\n",
    "        \n",
    "        ID = f'0-{i:06}'\n",
    "    \n",
    "        urls = get_urls(ID, f'database/0.csv')\n",
    "        reviews = scrape_reviews(urls)\n",
    "    \n",
    "        adress = f'database/reviews/0-{i:06}.csv'\n",
    "        l = save_to_csv(reviews, adress)\n",
    "        print(f'Saved {l} individual reviews in database/reviews/0-{ID}.csv\\n------------------------------------------------------------------')\n",
    "    \n",
    "    end_time = time.perf_counter()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f'Time taken: {elapsed_time:.2f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_dataframes = [] \n",
    "for j in range(279):\n",
    "    \n",
    "    \n",
    "    file_path = os.path.join('', f'database/reviews/0-{j:06}.csv')\n",
    "    # Чтение CSV файла и добавление его в список\n",
    "    try:\n",
    "        a = pd.read_csv(file_path)\n",
    "        testing_dataframes.append(a)\n",
    "\n",
    "    except:\n",
    "        print(f'No data in database/reviews/0-{j:06}.csv')\n",
    "# Объединение всех DataFrame в один\n",
    "\n",
    "#a = pd.read_csv('normal/reviews_filtered.csv')\n",
    "#testing_dataframes.append(a)\n",
    "\n",
    "testing_combined_df = pd.concat(testing_dataframes, ignore_index=True)\n",
    "testing_combined_df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54660"
      ]
     },
     "execution_count": 516,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_combined_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: I absolutely love this product! It has changed my life for the better.\n",
      "Sentiment: Positive (Polarity: 0.5625)\n",
      "\n",
      "Review: This is the worst purchase I have ever made. Completely disappointed.\n",
      "Sentiment: Negative (Polarity: -0.875)\n",
      "\n",
      "Review: The product is okay, not great but not terrible either.\n",
      "Sentiment: Positive (Polarity: 0.19999999999999998)\n",
      "\n",
      "Review: Amazing quality! Exceeded my expectations.\n",
      "Sentiment: Positive (Polarity: 0.7500000000000001)\n",
      "\n",
      "Review: Terrible service. Will not buy again.\n",
      "Sentiment: Negative (Polarity: -1.0)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from textblob import TextBlob\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def analyze_sentiment(review):\n",
    "\n",
    "    doc = nlp(review) # tokenizer \n",
    "    \n",
    "    processed_review = \" \".join([token.text for token in doc]) # adjusting for TextBlob\n",
    "\n",
    "    blob = TextBlob(processed_review)\n",
    "    \n",
    "    polarity = blob.sentiment.polarity # getting polarity between -1.0 and 1.0\n",
    "    return polarity\n",
    "\n",
    "reviews = [\n",
    "    \"I absolutely love this product! It has changed my life for the better.\",\n",
    "    \"This is the worst purchase I have ever made. Completely disappointed.\",\n",
    "    \"The product is okay, not great but not terrible either.\",\n",
    "    \"Amazing quality! Exceeded my expectations.\",\n",
    "    \"Terrible service. Will not buy again.\"\n",
    "]\n",
    "\n",
    "for review in reviews:\n",
    "    polarity = analyze_sentiment(review)\n",
    "    sentiment = \"Positive\" if polarity > 0 else \"Negative\" if polarity < 0 else \"Neutral\"\n",
    "    print(f\"Review: {review}\\nSentiment: {sentiment} (Polarity: {polarity})\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "def add_to_2(goods_url, reviews_url, filename=f'database/2.csv'):\n",
    "    if os.path.exists(filename):\n",
    "        df = pd.read_csv(filename)\n",
    "    else:\n",
    "        df = pd.DataFrame(columns=['ID', 'goods_url', 'reviews_url'])\n",
    "    \n",
    "    next_id = os.path.join('', f'2-{len(df):06}')\n",
    "    \n",
    "    new_row = pd.DataFrame([{'ID': next_id, 'goods_url': goods_url, 'reviews_url': reviews_url}])\n",
    "    \n",
    "    if not df[(df['goods_url'] == goods_url) & (df['reviews_url'] == reviews_url)].empty:\n",
    "        print(f\"Duplicate row found. The row {goods_url} will not be added to {filename}.\")\n",
    "        return\n",
    "    \n",
    "    df = pd.concat([df, new_row], ignore_index=True)\n",
    "    \n",
    "    df.to_csv(filename, index=False, encoding='utf-8')\n",
    "    \n",
    "    print(f\"Row {goods_url} added successfully to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_to_2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
